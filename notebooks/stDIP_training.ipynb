{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "093e82c2-51be-439a-8887-a8822b34184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"False\"\n",
    "os.environ[\"JAX_LOG_COMPILES\"] = \"0\"\n",
    "\n",
    "import sys\n",
    "import jax.numpy as np\n",
    "from jax import vmap\n",
    "\n",
    "import numpy as onp\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # disables GUI backend\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from jax import jit, random\n",
    "from inrmri.new_radon import get_weight_freqs, make_forward_radon_operator\n",
    "from inrmri.basic_nn import weighted_loss \n",
    "from inrmri.utils import denoise_loss_batch, l1_loss, tikhonov_loss, tv_loss, create_center_mask\n",
    "import optax \n",
    "from inrmri.advanced_training import OptimizerWithExtraState, train_with_updates_ms_nspokeswise_select\n",
    "import itertools\n",
    "from inrmri.utils_rdls import get_shedule, get_predim_direct_ms\n",
    "from inrmri.utils_rdls import create_folder, save_frames_as_gif_with_pillow\n",
    "from inrmri.utils_rdls import get_varying_keys, config_to_foldername\n",
    "from inrmri.utils_rdls import plot_curves, plot_curves_and_mins, plot_multi_axis, multiple_images_visualization\n",
    "from inrmri.dip import MS_TD_DIP_Net, multi_slice_circle_generator\n",
    "from inrmri.utils_rdls import safe_normalize, get_center, pad_axis_to_length\n",
    "from inrmri.utils import clear_jax_memory # parse_slices_list\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be5159f7-e119-47a7-90f2-48eaa92ee256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# volunteer parameters\n",
    "volunteer = 'MP'\n",
    "dataset = 'DATA_0.55T'\n",
    "total_slices = 8\n",
    "base_path = \"C:\\\\Users\\\\Legion\\\\Documents\\\\Repositories\\\\NF-cMRI-rafa\\\\datasets\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb528485-2f66-4c6c-aeb1-e259e4473dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DATA ---\n",
    "base_folder           = base_path + dataset + '\\\\' + volunteer + '\\\\'\n",
    "train_data_folder     = base_folder + 'traindata\\\\'\n",
    "model_path            = base_folder + 'stDIP\\\\'\n",
    "create_folder(model_path, reset=False)\n",
    "\n",
    "# static parameters\n",
    "dummy_iterations = 0 # iterations that are not shown in metrics\n",
    "num_frames       = 30\n",
    "saturation       = 0.3\n",
    "n_coils          = 15\n",
    "\n",
    "# training parameters\n",
    "slices_list      = [1,2,3,4,5,6,7,8]\n",
    "n_slices         = len(slices_list)\n",
    "s_idxs           = [s_idx - 1 for s_idx in slices_list]\n",
    "perform_plots    = True\n",
    "val_frames       = np.array([0]  , dtype=np.int32 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5f65d41-d131-4e99-a964-eb22deb53ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PARMETERS ---\n",
    "key = random.PRNGKey(0)\n",
    "key_net, key_params, key_train, key_latent = random.split(key,4) # keys for reproducibility\n",
    "key_eval_list = random.split(key, total_slices)\n",
    "\n",
    "experiment_params = {              \n",
    "    'N':                    [256],                 # elements of the readout, lenght of the spoke\n",
    "    'mapnet_layers':        [16],                  # MapNet: layers\n",
    "    'cnn_latent_shape':     [8],                   # Generatior: input shape, is the trained latent representation by MapNet\n",
    "    'levels':               [4],                   # Generatior: levels of up sampling\n",
    "    'features':             [16],                  # Generatior: features\n",
    "    'iter':                 [500],                # number of iterations\n",
    "    'bs':                   [1],                   # Number of cardiac phases per iteration (real batch size: bs * nspokes * nslices)\n",
    "    'addConst':             [False],               # if add a constant to the fixed manifold\n",
    "    'str_filter':           ['ramp'],              # frequency weighting\n",
    "    'denoise_type':         ['tv'],                # Regularization ('tv', 'l1', 'tikhonov')\n",
    "    'lambda':               [0],                   # lambda for regularization\n",
    "    'lr_schedule':          ['constant_schedule'], # learning rate scheduler\n",
    "    'lr_init_value':        [5e-3],                # learninig rate start (or constant value depending on the scheduler)\n",
    "    'lr_end':               [1e-3],                # learning rate end (optional depending on the scheduler)\n",
    "    'lr_transition_steps':  [3000],           # how many steps to decay over\n",
    "    'lr_decay_rate':        [0.90],           # (parameter for 'exponential_decay' scheduler)\n",
    "    'lr_power':             [20],             # (parameter for 'polynomial_schedule' scheduler)\n",
    "    'metric_step':          [20],             # step of iterations to compute metrics\n",
    "    'window_size':          [5],              # number of elements in windows to compute window metrics (variance)\n",
    "    'nspokes':              [1],              # number of spokes used of each frame in the nspokes-wise training\n",
    "    'select_by':           ['loss', 'mean_var'],  # The used metric for selecting the best parameters\n",
    "    'nr_iqm':               [True],           # No-reference image quality metric (WMV)\n",
    "    'fr_iqm':               [True],           # Full-reference image quality metric (PSNR, SSIM, AP)\n",
    "    'debug':                [False],          # If debug, multiple shapes and time should be printed\n",
    "    'data_ponderator':      [1],              # currently the data is normalized, so this ponderator would ponderate a normalized data\n",
    "    'latent_r':             [1],              # radius of the latent representation\n",
    "    'latent_z':             [10],             # the latent representation creates circles between [-Z_max, Z_max]\n",
    "}\n",
    "# Generate list of parameter combinations as dictionaries\n",
    "keys = list(experiment_params.keys())\n",
    "combinations = [dict(zip(keys, values)) for values in itertools.product(*experiment_params.values())]\n",
    "# Identify the varying keys\n",
    "varying_keys = get_varying_keys(combinations)\n",
    "experiment_name = '_'.join(varying_keys)\n",
    "# plot variables\n",
    "xlim_botton = dummy_iterations\n",
    "xlim_top    = int(np.max(np.array(experiment_params['iter'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66fa00f8-bd8d-4d99-93e8-4c78d4fbad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- create the multislice dataset ---\n",
    "Y_data_list      =   []\n",
    "X_data_list      =   []\n",
    "csm_list         =   []\n",
    "spclim_list      =   []\n",
    "hollow_mask_list =   []\n",
    "recon_fs_list    =   []\n",
    "exp_folder_path_list  =   []\n",
    "for  slice_num in slices_list:\n",
    "    dataset_name = 'slice_' + str(slice_num) + '_' + str(total_slices) +'_nbins' + str(num_frames)\n",
    "    path_file = train_data_folder + dataset_name + '.npz'\n",
    "    data = onp.load(    path_file         )\n",
    "    y_data_item = data['Y_data']\n",
    "    x_data_item = data['X_data']\n",
    "    csm_item    = data['csm']\n",
    "    spclim_item = data['spclim']\n",
    "    y_data_item = data['Y_data']\n",
    "    if dataset == 'DATA_0.55T':\n",
    "        hollow_mask_item  = data['hollow_mask']    \n",
    "    else:\n",
    "        hollow_mask_item  = data['hollow_mask_computed']   \n",
    "    # Check coils\n",
    "    y_data_item         = pad_axis_to_length(y_data_item, 1, n_coils, pad_value=0+0j)\n",
    "    csm_item            = pad_axis_to_length(csm_item, 0, n_coils, pad_value=0+0j)\n",
    "    # append items in lists\n",
    "    Y_data_list.append(       y_data_item    )\n",
    "    X_data_list.append(       x_data_item    )  \n",
    "    csm_list.append(          csm_item      )\n",
    "    spclim_list.append(       spclim_item   ) \n",
    "    hollow_mask_list.append(  hollow_mask_item   ) \n",
    "    recon_fs    =  data['recon_fs']\n",
    "    recon_fs    = get_center(recon_fs) \n",
    "    recon_fs    = safe_normalize(recon_fs) \n",
    "    recon_fs_list.append(   recon_fs[:,:,val_frames]   )\n",
    "    save_folder  = model_path + dataset_name.replace(\".\", \"_\") + '\\\\'\n",
    "    create_folder(save_folder, reset=False )\n",
    "    save_frames_as_gif_with_pillow(save_folder, recon_fs, filename='recon_fs', vmax=1, saturation=saturation, fps=30)\n",
    "    exp_folder_path = save_folder + experiment_name + '\\\\'\n",
    "    exp_folder_path_list.append(  exp_folder_path  )\n",
    "    create_folder(exp_folder_path, reset=False)\n",
    "hollow_mask_array = np.stack(hollow_mask_list, axis=0)\n",
    "\n",
    "## --- Radon operator list ---\n",
    "radon_operator_list = []\n",
    "for jj in range(len(csm_list)):\n",
    "    radon_operator_list.append( make_forward_radon_operator(csm_list[jj], spclim_list[jj]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea30d0d6-ea54-450f-8e4f-58018ebb3309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRACING] Recompiling tDIP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train iter:   0%|          | 0/501 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRACING] Recompiling tDIP...\n",
      "256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train iter:   0%|          | 1/501 [00:24<3:22:51, 24.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRACING] Recompiling tDIP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train iter: 100%|██████████| 501/501 [07:57<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRACING] Recompiling tDIP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\legion\\onedrive - escuela politécnica nacional\\postdoc_ihealth\\st-dip-sr\\repositories\\nf-cmri\\inrmri\\utils_rdls.py:794: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRACING] Recompiling tDIP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train iter:   0%|          | 0/501 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRACING] Recompiling tDIP...\n",
      "256\n",
      "[TRACING] Recompiling tDIP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train iter: 100%|██████████| 501/501 [08:59<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRACING] Recompiling tDIP...\n"
     ]
    }
   ],
   "source": [
    "# --- Combinations LOOP ---\n",
    "training_names_list = []\n",
    "experiment_records = {}\n",
    "for s_idx in s_idxs:\n",
    "    experiment_records[s_idx] = []\n",
    "log_list = []\n",
    "\n",
    "for i, h_params in enumerate(combinations):\n",
    "    # add information\n",
    "    h_params['NFRAMES']    = num_frames\n",
    "    h_params['n_slices']   = n_slices\n",
    "    h_params['val_frames'] = val_frames\n",
    "    # training name\n",
    "    training_name = config_to_foldername(h_params,varying_keys)\n",
    "    training_names_list.append(  training_name  )\n",
    "    training_folder_list = []\n",
    "    for s_idx in s_idxs:\n",
    "        training_folder = exp_folder_path_list[s_idx] + training_name + '/'\n",
    "        create_folder(training_folder, reset=False)\n",
    "        training_folder_list.append(  training_folder  )\n",
    "    # model config\n",
    "    CONFIG_NET = {\n",
    "        'mapnet_layers':    [h_params['mapnet_layers'], h_params['mapnet_layers'],],\n",
    "        'cnn_latent_shape': (h_params['cnn_latent_shape'],h_params['cnn_latent_shape']),\n",
    "        'levels':            h_params['levels'],\n",
    "        'features':          h_params['features']\n",
    "    }\n",
    "    # data ponderation\n",
    "    Y_data_list_current     =  []\n",
    "    recon_fs_list_current   =  []\n",
    "    for i in range(len(Y_data_list)):\n",
    "        Y_data_list_current.append(     Y_data_list[i]    *  h_params['data_ponderator']   )\n",
    "        recon_fs_list_current.append(   recon_fs_list[i]  ) #*  h_params['data_ponderator']   )\n",
    "\n",
    "    # define the network\n",
    "    net = MS_TD_DIP_Net(\n",
    "        nframes = h_params['NFRAMES'],\n",
    "        addConst = h_params['addConst'], \n",
    "        key_latent = key_latent,\n",
    "        n_slices = h_params['n_slices'],\n",
    "        latent_generator=multi_slice_circle_generator,\n",
    "        imshape = [h_params['N'],h_params['N']],\n",
    "        radius = h_params['latent_r'],\n",
    "        z_min = -h_params['latent_z'],\n",
    "        z_max = h_params['latent_z'],\n",
    "        **CONFIG_NET\n",
    "    )\n",
    "    params = net.init_params(key_params) \n",
    "    WEIGHT_FREQS = get_weight_freqs(  h_params['N'], h_params['str_filter']  )\n",
    "    weights        = (1. + WEIGHT_FREQS)[None, None, :]\n",
    "    schedule       = get_shedule(h_params)\n",
    "    optimizer      = OptimizerWithExtraState(optax.adam(learning_rate=schedule))\n",
    "    lambda_denoise_reg = h_params['lambda']\n",
    "    center_mask    = create_center_mask((h_params['N'],h_params['N']))\n",
    "    if h_params['denoise_type'] == 'tv':\n",
    "        den_loss = tv_loss\n",
    "    elif h_params['denoise_type'] == 'tikhonov':\n",
    "        den_loss = tikhonov_loss\n",
    "    elif h_params['denoise_type'] == 'l1':\n",
    "        den_loss = l1_loss\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown denoise_type: {h_params['denoise_type']}\")\n",
    "\n",
    "    def loss(params, X_list, Y_list, index_frames, key):\n",
    "        n_slices = len(X_list)\n",
    "        total_loss = 0.0\n",
    "        def per_spoke_loss(im, x_spoke, y_spoke, radon_operator):\n",
    "            # (im): (1, px, py), (x_spoke): scalar, (y_spoke): (cmap, nx)\n",
    "            alphas_frame = x_spoke[0:1]  # shape (1,)\n",
    "            y_data = y_spoke[..., 0]     # shape (cmap, nx)\n",
    "            pred_kspace = radon_operator(im, alphas_frame)\n",
    "            loss_val = weighted_loss(pred_kspace, y_data, weights)\n",
    "            return loss_val\n",
    "    \n",
    "        def per_frame_loss(im, X_frame, Y_frame, radon_operator):\n",
    "            # X_frame: (nspokes, features), Y_frame: (nspokes, cmap, nx, 1)\n",
    "            spoke_loss_fn = lambda x_sp, y_sp: per_spoke_loss(im, x_sp, y_sp, radon_operator)\n",
    "            loss_vals = vmap(spoke_loss_fn)(X_frame, Y_frame)\n",
    "            return np.sum(loss_vals)\n",
    "    \n",
    "        for slice_idx in range(n_slices):\n",
    "            X = X_list[slice_idx]              # (frames, spokes, features)\n",
    "            Y = Y_list[slice_idx]              # (frames, spokes, cmap, nx, 1)\n",
    "            radon_operator = radon_operator_list[slice_idx]\n",
    "    \n",
    "            ims, updates = net.train_forward_pass(params, key, index_frames, slice_idx)  # (frames, px, py)\n",
    "    \n",
    "            def frame_loss_fn(im, Xf, Yf):\n",
    "                im_exp = im[None, ...]  # (1, px, py)\n",
    "                frame_loss = per_frame_loss(im_exp, Xf, Yf, radon_operator)\n",
    "                frame_loss += lambda_denoise_reg * denoise_loss_batch(im_exp[0, :, :][..., None], den_loss, center_mask)\n",
    "                return frame_loss\n",
    "            \n",
    "            total_loss += np.sum(vmap(frame_loss_fn)(ims, X, Y))\n",
    "        total_loss /= n_slices\n",
    "        return total_loss, updates\n",
    "\n",
    "    # this function uses the model to reconstruct frames for metrc computations and visualizations during training\n",
    "    def recon_cine(params, t_idx, s_idx, hollow_mask, key):\n",
    "        # Forward pass — should already output JAX arrays\n",
    "        ims, _ = net.train_forward_pass(params, key, t_idx, s_idx)     # (frames, px, py)\n",
    "        # Efficient axis permutation — avoid moveaxis in JAX hot loops\n",
    "        ims = ims.transpose(1, 2, 0)  # (px, py, frames)\n",
    "        # Apply mask — (1 - hollow_mask) is broadcasted safely\n",
    "        ims = ims * (1.0 - hollow_mask)[..., None]\n",
    "        # Crop center — ensure JAX-native `get_center`\n",
    "        ims = get_center(ims)  # Output: (px//2, py//2, frames)\n",
    "        # Normalize — assume JAX-safe operation\n",
    "        ims = safe_normalize(ims)\n",
    "        return ims\n",
    "        \n",
    "    # training\n",
    "    loss_fn = jit(loss)\n",
    "    recon_fn = jit(recon_cine)\n",
    "    if h_params['select_by'] == 'mean_var':\n",
    "        results = train_with_updates_ms_nspokeswise_select(loss_fn, X_data_list, Y_data_list_current, params, optimizer, key_train, h_params, recon_cine=recon_fn, hollow_mask_array=hollow_mask_array, val_slices=s_idxs, reference_list = recon_fs_list_current, debug=h_params['debug'])\n",
    "    elif h_params['select_by'] == 'loss':\n",
    "        results = train_with_updates_ms_nspokeswise_select(loss_fn, X_data_list, Y_data_list_current, params, optimizer, key_train, h_params, debug=h_params['debug'])\n",
    "    # --- memory management (post-training)---\n",
    "    del optimizer, schedule, WEIGHT_FREQS, weights\n",
    "    clear_jax_memory()\n",
    "    # save records\n",
    "    for s_idx in s_idxs:\n",
    "        record = {key: h_params[key] for key in varying_keys}\n",
    "        record['duration [s]']  = results['time_s']\n",
    "        record['duration [min]']  = results['time_s'] / 60\n",
    "        record['duration']      = results['time_str']\n",
    "        # First dictionary-level metrics\n",
    "        for var_name in ['it', 'loss', 'mean_var', 'time']:\n",
    "            if var_name in results.get('best', {}):\n",
    "                record[var_name] = results['best'][var_name]\n",
    "        \n",
    "        # Second-level metrics indexed by s_idx\n",
    "        best = results.get('best', {})\n",
    "        if isinstance(best, dict) and s_idx in best:\n",
    "            sub_metrics = best[s_idx]\n",
    "            if isinstance(sub_metrics, dict):\n",
    "                for var_name in ['var', 'ssim', 'psnr', 'ap']:\n",
    "                    if var_name in sub_metrics:\n",
    "                        record[var_name] = sub_metrics[var_name]\n",
    "        record['experiment_folder'] = exp_folder_path_list[s_idx]  # optional: keep the folder name\n",
    "        record['training_name']     = training_name\n",
    "        record['training_folder']   = exp_folder_path_list[s_idx] + training_name + '/'\n",
    "        experiment_records[s_idx].append(record)\n",
    "    # log \n",
    "    if perform_plots:\n",
    "        log_list.append(  results['log_queue']  )\n",
    "    # save best image\n",
    "    for s_idx in s_idxs:\n",
    "        best_recon   =  get_predim_direct_ms(net, hollow_mask_array[s_idx], key_eval_list[s_idx], h_params, s_idx, results['best']['params'] )\n",
    "        np.savez(training_folder_list[s_idx] + 'best_recon' + '.npz', **{'best_recon':best_recon,  **h_params, **record})\n",
    "        np.savez(training_folder_list[s_idx] + 'log_queue'  + '.npz', **results['log_queue'][s_idx])\n",
    "        save_frames_as_gif_with_pillow(training_folder_list[s_idx], best_recon, filename='best_recon', vmax=1, saturation=saturation, fps=num_frames)\n",
    "        if perform_plots:\n",
    "            log_queue = results.get('log_queue', {})\n",
    "        \n",
    "            def safe_get_metric(metric):\n",
    "                return (\n",
    "                    s_idx in log_queue and \n",
    "                    isinstance(log_queue[s_idx], dict) and \n",
    "                    metric in log_queue[s_idx]\n",
    "                )\n",
    "        \n",
    "            # NR-IQM plots and images\n",
    "            if h_params.get('nr_iqm', False):\n",
    "                if 'it' in log_queue and safe_get_metric('var'):\n",
    "                    plot_multi_axis(\n",
    "                        log_queue[s_idx]['it'],\n",
    "                        [log_queue[s_idx]['var']],\n",
    "                        ['VAR'],\n",
    "                        xlim=(xlim_botton, h_params['iter']),\n",
    "                        save_path=training_folder_list[s_idx] + 'nr_iqm_curves.png'\n",
    "                    )\n",
    "        \n",
    "                if 'best' in results and 'it' in results['best']:\n",
    "                    multiple_images_visualization(\n",
    "                        [recon_fs, best_recon],\n",
    "                        ['GT', 'VAR (it' + str(results['best']['it']) + ')'],\n",
    "                        frame=0,\n",
    "                        save_path=training_folder_list[s_idx] + 'nr_iqm_images.png',\n",
    "                        saturation=saturation\n",
    "                    )\n",
    "        \n",
    "            # FR-IQM curves\n",
    "            if h_params.get('fr_iqm', False):\n",
    "                metrics = ['psnr', 'ssim', 'ap']\n",
    "                available_metrics = [m for m in metrics if safe_get_metric(m)]\n",
    "        \n",
    "                if 'it' in log_queue and available_metrics:\n",
    "                    plot_multi_axis(\n",
    "                        log_queue[s_idx]['it'],\n",
    "                        [log_queue[s_idx][m] for m in available_metrics],\n",
    "                        [m.upper() for m in available_metrics],\n",
    "                        xlim=(xlim_botton, h_params['iter']),\n",
    "                        save_path=training_folder_list[s_idx] + 'fr_iqm_curves.png'\n",
    "                    )\n",
    "    # --- memory management (combinatory-level) ---\n",
    "    del net, results, best_recon\n",
    "    clear_jax_memory()\n",
    "    plt.close('all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33a30046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\legion\\onedrive - escuela politécnica nacional\\postdoc_ihealth\\st-dip-sr\\repositories\\nf-cmri\\inrmri\\utils_rdls.py:777: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "for s_idx in s_idxs:\n",
    "    # save dataset\n",
    "    df = pd.DataFrame(experiment_records[s_idx])\n",
    "    df.to_csv(exp_folder_path_list[s_idx] + \"experiment_records.csv\", index=False, sep=';')\n",
    "    if perform_plots:\n",
    "        log_list_slice = []\n",
    "        for com_idx in range(len(log_list)):\n",
    "            item = log_list[com_idx][s_idx]\n",
    "            log_list_slice.append(item)\n",
    "    \n",
    "        def variable_exists(log_list_slice, var_name):\n",
    "            return all(var_name in item for item in log_list_slice)\n",
    "    \n",
    "        # Plot 'loss' curve if available\n",
    "        if variable_exists(log_list_slice, 'loss'):\n",
    "            image_path = exp_folder_path_list[s_idx] + 'curves_loss.png'\n",
    "            plot_curves_and_mins(log_list_slice, training_names_list, 'loss', save_path=image_path, xlim=(xlim_botton, xlim_top))\n",
    "    \n",
    "        # Plot 'var' curve if required and available\n",
    "        if h_params['nr_iqm'] and variable_exists(log_list_slice, 'var'):\n",
    "            image_path = exp_folder_path_list[s_idx] + 'curves_var.png'\n",
    "            plot_curves_and_mins(log_list_slice, training_names_list, 'var', save_path=image_path, xlim=(xlim_botton, xlim_top))\n",
    "    \n",
    "        # Plot 'ssim', 'psnr', 'ap' if required and available\n",
    "        if h_params['fr_iqm']:\n",
    "            for r_iqm in ['ssim', 'psnr', 'ap']:\n",
    "                if variable_exists(log_list_slice, r_iqm):\n",
    "                    image_path = exp_folder_path_list[s_idx] + f'curves_{r_iqm}.png'\n",
    "                    plot_curves(log_list_slice, training_names_list, r_iqm, save_path=image_path, xlim=(xlim_botton, xlim_top))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24ba204-cbdc-41d6-92fd-66b7471e9b9b",
   "metadata": {},
   "source": [
    "## Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e03097f-7cdc-4a45-85f3-4fe2a9121e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after all, create a overall dataset\n",
    "metrics_for_analysis = ['var', 'ssim', 'psnr', 'ap']\n",
    "metrics_list_for_analysis = ['log_var', 'log_ssim', 'log_psnr', 'log_ap', 'log_loss', 'log_time']\n",
    "stay_the_same = ['duration [s]', 'duration [min]', 'duration', 'it', 'log_it', 'training_name']\n",
    "variables = varying_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4b3ec73-3130-450f-b525-caee7b79b4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_group(group):\n",
    "    result = {}\n",
    "\n",
    "    # 1. Scalar metrics → compute mean and std\n",
    "    for metric in metrics_for_analysis:\n",
    "        result[f'{metric}_mean'] = group[metric].mean()\n",
    "        result[f'{metric}_std']  = group[metric].std()\n",
    "\n",
    "    # 2. List-valued metrics → parse, stack, compute mean/std per position\n",
    "    for col in metrics_list_for_analysis:\n",
    "        try:\n",
    "            lists = group[col].apply(ast.literal_eval).tolist()\n",
    "            arr = np.array(lists, dtype=np.float32)\n",
    "            result[f'{col}_mean'] = arr.mean(axis=0).tolist()\n",
    "            result[f'{col}_std']  = arr.std(axis=0).tolist()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {col}: {e}\")\n",
    "            result[f'{col}_mean'] = np.nan\n",
    "            result[f'{col}_std']  = np.nan\n",
    "\n",
    "    # 3. Keep the first value of \"stay_the_same\" columns\n",
    "    for col in stay_the_same:\n",
    "        result[col] = group[col].iloc[0]\n",
    "\n",
    "    return pd.Series(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50e83d1b-12e8-4e2f-9121-273e0f4d0ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create general dataset\n",
    "count = 0\n",
    "for i in slices_list:\n",
    "    experiment_path = model_path + 'slice_'+str(i)+'_'+str(total_slices)+'_nbins30' + '/'+experiment_name   \n",
    "    csv_path = experiment_path + \"/experiment_records.csv\"\n",
    "    if count == 0:\n",
    "        df_slices = pd.read_csv(csv_path, delimiter=';')\n",
    "        df_slices['slice'] = i\n",
    "        count+=1\n",
    "    else:\n",
    "        df = pd.read_csv(csv_path, delimiter=';')\n",
    "        df['slice'] = i\n",
    "        df_slices = pd.concat([df_slices, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30367022-eac2-4c6f-a700-fa24ef6ea813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing log_var: malformed node or string: nan\n",
      "Error processing log_ssim: malformed node or string: nan\n",
      "Error processing log_psnr: malformed node or string: nan\n",
      "Error processing log_ap: malformed node or string: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Legion\\AppData\\Local\\Temp\\ipykernel_29912\\1361453854.py:8: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  summary_df = df_total.groupby(variables).apply(process_group).reset_index()\n"
     ]
    }
   ],
   "source": [
    "df_total = df_slices.reset_index(drop=True)\n",
    "for index, row in df_total.iterrows():\n",
    "    log_path = row['training_folder'] + 'log_queue.npz'\n",
    "    log_queue = np.load(log_path)\n",
    "    for key  in log_queue.keys():    \n",
    "        df_total.at[index, 'log_'+key] = str(log_queue[key].tolist())\n",
    "df_total.to_csv(model_path + experiment_name + \".csv\" , index=False, sep=';')\n",
    "summary_df = df_total.groupby(variables).apply(process_group).reset_index()\n",
    "summary_df.to_csv(model_path + experiment_name + \"_summary.csv\" , index=False, sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
